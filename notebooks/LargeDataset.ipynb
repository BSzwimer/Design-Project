{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LargeDataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "694c69a576094bfd9d101e3b44cb6232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0c1302cdaaf84a61a229f2831d45ecbb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_23582fed32c947f58922e2596c4d6c1c",
              "IPY_MODEL_6844ed55998a4d45a5b7a2a102c49ceb"
            ]
          }
        },
        "0c1302cdaaf84a61a229f2831d45ecbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "23582fed32c947f58922e2596c4d6c1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_065636a849a84c78b7c7316427559148",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 405234788,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 405234788,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_83f5bb7784754803b7d3e9b1e2461dd6"
          }
        },
        "6844ed55998a4d45a5b7a2a102c49ceb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c3bb3bb5047e40c4bb528e3fc57800f8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 405M/405M [00:27&lt;00:00, 15.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b005f8bbfdf841c5ad52aa5c4d490742"
          }
        },
        "065636a849a84c78b7c7316427559148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "83f5bb7784754803b7d3e9b1e2461dd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c3bb3bb5047e40c4bb528e3fc57800f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b005f8bbfdf841c5ad52aa5c4d490742": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "694c69a576094bfd9d101e3b44cb6232",
            "0c1302cdaaf84a61a229f2831d45ecbb",
            "23582fed32c947f58922e2596c4d6c1c",
            "6844ed55998a4d45a5b7a2a102c49ceb",
            "065636a849a84c78b7c7316427559148",
            "83f5bb7784754803b7d3e9b1e2461dd6",
            "c3bb3bb5047e40c4bb528e3fc57800f8",
            "b005f8bbfdf841c5ad52aa5c4d490742"
          ]
        },
        "id": "DElTuIgs0eOk",
        "outputId": "0294249e-5394-4ab0-dd1f-de22ea02b78d"
      },
      "source": [
        "\"\"\"\n",
        "To import all the dependencies run this block, then restart the runtime and run again\n",
        "\"\"\"\n",
        "\n",
        "!pip install rake-nltk\n",
        "!python -m spacy_grammar.grammar\n",
        "!pip install --upgrade language_tool_python\n",
        "!python -m spacy download en_core_web_lg\n",
        "!pip install wikipedia\n",
        "!pip install bs4\n",
        "!pip install sentence-transformers\n",
        "!pip3 install torch torchvision\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install --upgrade azure-cognitiveservices-vision-computervision\n",
        "\n",
        "import requests\n",
        "import spacy\n",
        "import nltk\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import re\n",
        "import language_tool_python\n",
        "import numpy as np\n",
        "import wikipedia\n",
        "import csv\n",
        "import torch\n",
        "import clip\n",
        "import pickle\n",
        "\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "from array import array\n",
        "from PIL import Image\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import stopwords \n",
        "from rake_nltk import Rake\n",
        "from __future__ import unicode_literals\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import namedtuple\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
        "from azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "tool = language_tool_python.LanguageTool('en-US')\n",
        "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "subscription_key = \"143964306b7e4cea8f7750613ce3e7de\"\n",
        "endpoint = \"https://attack.cognitiveservices.azure.com/\"\n",
        "\n",
        "computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "openai_model, openai_preprocess = clip.load(\"ViT-B/32\", device=device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rake-nltk in /usr/local/lib/python3.7/dist-packages (1.0.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rake-nltk) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->rake-nltk) (1.15.0)\n",
            "/usr/bin/python3: Error while finding module specification for 'spacy_grammar.grammar' (ModuleNotFoundError: No module named 'spacy_grammar')\n",
            "Requirement already up-to-date: language_tool_python in /usr/local/lib/python3.7/dist-packages (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from language_tool_python) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from language_tool_python) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (2020.12.5)\n",
            "Requirement already satisfied: en_core_web_lg==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz#egg=en_core_web_lg==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (54.1.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (1.0.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: transformers<5.0.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.95)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.10.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.7.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.7.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (5.9)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-9051b6qf\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-9051b6qf\n",
            "Requirement already satisfied (use --upgrade to upgrade): clip==1.0 from git+https://github.com/openai/CLIP.git in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (5.9)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.41.1)\n",
            "Requirement already satisfied: torch~=1.7.1 in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.7.1)\n",
            "Requirement already satisfied: torchvision~=0.8.2 in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.8.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch~=1.7.1->clip==1.0) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch~=1.7.1->clip==1.0) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision~=0.8.2->clip==1.0) (7.0.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-cp37-none-any.whl size=1368708 sha256=fa3b2a4c2ee434bdd8694233bb31fda219488bc855627d51d31755e5266d9a5b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-j89i7z6v/wheels/79/51/d7/69f91d37121befe21d9c52332e04f592e17d1cabc7319b3e09\n",
            "Successfully built clip\n",
            "Requirement already up-to-date: azure-cognitiveservices-vision-computervision in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied, skipping upgrade: msrest>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from azure-cognitiveservices-vision-computervision) (0.6.21)\n",
            "Requirement already satisfied, skipping upgrade: azure-common~=1.1 in /usr/local/lib/python3.7/dist-packages (from azure-cognitiveservices-vision-computervision) (1.1.27)\n",
            "Requirement already satisfied, skipping upgrade: isodate>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: requests~=2.16 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from isodate>=0.6.0->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (3.1.0)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading LanguageTool: 100%|██████████| 190M/190M [00:16<00:00, 11.2MB/s]\n",
            "Unzipping /tmp/tmp2badgpwm.zip to /root/.cache/language_tool_python.\n",
            "Downloaded https://www.languagetool.org/download/LanguageTool-5.2.zip to /root/.cache/language_tool_python.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "694c69a576094bfd9d101e3b44cb6232",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=405234788.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████| 354M/354M [00:01<00:00, 221MiB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbZnYroohKfO"
      },
      "source": [
        "page_names = [\n",
        "  'Egyptian_temple',\n",
        "  'Connecticut_Tercentenary_half_dollar',\n",
        "  'Eastern_green_mamba',\n",
        "  'Barack',\n",
        "  'Mount_Tambora',\n",
        "  'Hans_Eberle',\n",
        "  'Fennel',\n",
        "  '3rd_Light_Horse_Brigade',\n",
        "  'Liberal_feminism',\n",
        "  'Presidency_of_Joe_Biden',\n",
        "  'Blacksmith',\n",
        "  '7_World_Trade_Center',\n",
        "  'Michigan_State_Capitol',\n",
        "  'Santa_María_de_Óvila',\n",
        "  'Annunciation_(Memling)',\n",
        "  'Felice_Beato',\n",
        "  'Clathrus_ruber',\n",
        "  'Green_rosella',\n",
        "  'Psittacosaurus',\n",
        "  'Lafayette_dollar',\n",
        "  'Megalodon',\n",
        "  'Ficus_obliqua',\n",
        "  'Hu_Zhengyan',\n",
        "  'Olmec_colossal_heads',\n",
        "  'Warkworth_Castle',\n",
        "  'Carnivàle',\n",
        "  'H.M.S._Pinafore',\n",
        "  'Magnetosphere_of_Jupiter',\n",
        "  '2015_Formula_One_World_Championship',\n",
        "  'Ruma_Maida',\n",
        "  'Wells_Cathedral',\n",
        "  'Introduction_to_viruses',\n",
        "  'Fork-marked_lemur',\n",
        "  'Salvia_yangii',\n",
        "  'Psilocybe_aztecorum',\n",
        "  'Springbok',\n",
        "  'Halkett_boat',\n",
        "  'Hoover_Dam',\n",
        "  'Rolls-Royce_R',\n",
        "  'Titchwell_Marsh',\n",
        "  'Imperial_Trans-Antarctic_Expedition',\n",
        "  'Parthian_Empire',\n",
        "  'Khalid_al-Mihdhar',\n",
        "  'Liber_Eliensis',\n",
        "  'Imaginative_Tales',\n",
        "  'Prometheus_(2012_film)',\n",
        "  'Typhoon_Gay_(1989)',\n",
        "  'Babylonia',\n",
        "  'March_2021_Australian_floods',\n",
        "  'Saint_Patrick%27s_Day',\n",
        "  'Elizabeth_II',\n",
        "  'Justice_League_(film)',\n",
        "  'The_Falcon_and_the_Winter_Soldier',\n",
        "  'Dua_Lipa',\n",
        "  'Billie_Eilish',\n",
        "  'Nine_Stones,_Winterbourne_Abbas',\n",
        "  'Palazzo_Pitti',\n",
        "  'Sloan–Parker_House',\n",
        "  'Mycena_haematopus',\n",
        "  'Lycoperdon_echinatum',\n",
        "  'Entoloma_sinuatum',\n",
        "  'Gyromitra_esculenta',\n",
        "  'Banksia_verticillata',\n",
        "  'Banksia_brownii',\n",
        "  'Ficus_macrophylla',\n",
        "  'Banksia_aculeata',\n",
        "  'Columbian_mammoth',\n",
        "  'Evolution_of_lemurs',\n",
        "  'Ferugliotherium',\n",
        "  'Killer_whale',\n",
        "  'Orangutan',\n",
        "  'Thomasomys_ucucha',\n",
        "  'Red-winged_fairywren',\n",
        "  'Garden_warbler',\n",
        "  'Puerto_Rican_amazon',\n",
        "  'Inaccessible_Island_rail',\n",
        "  'Echo_parakeet',\n",
        "  'Russet_sparrow',\n",
        "  'Red-capped_parrot',\n",
        "  'Pied_butcherbird',\n",
        "  'King_Island_emu',\n",
        "  'California_condor',\n",
        "  'Green_children_of_Woolpit',\n",
        "  'Polish_culture_during_World_War_II',\n",
        "  'Kylfings',\n",
        "  'British_National_(Overseas)',\n",
        "  'Baden-Powell_House',\n",
        "  'Anna_Anderson',\n",
        "  'Royal_National_College_for_the_Blind',\n",
        "  'Oriel_College,_Oxford',\n",
        "  'Construction_of_the_World_Trade_Center',\n",
        "  'Shale_oil_extraction',\n",
        "  'Shuttle–Mir_program',\n",
        "  'Nico_Ditch',\n",
        "  'Payún_Matrú',\n",
        "  'Larrys_Creek',\n",
        "  '1966_New_York_City_smog',\n",
        "  'Brown_Dog_affair',\n",
        "  'École_Polytechnique_massacre',\n",
        "  'The_Magazine_of_Fantasy_%26_Science_Fiction',\n",
        "  'Future_Science_Fiction_and_Science_Fiction_Stories',\n",
        "  'Famous_Fantastic_Mysteries',\n",
        "  'Science-Fiction_Plus',\n",
        "  'Super_Smash_Bros._Brawl',\n",
        "  'Metroid_Prime_3:_Corruption',\n",
        "  'God_of_War:_Betrayal',\n",
        "  'The_Simpsons_Game',\n",
        "  'Defense_of_the_Ancients',\n",
        "  'God_of_War_III',\n",
        "  'Honan_Chapel',\n",
        "  'Gevninge_helmet_fragment',\n",
        "  'Clemuel_Ricketts_Mansion',\n",
        "  'Castle',\n",
        "  'St_Helen%27s_Church,_Ashby-de-la-Zouch',\n",
        "  'Trump_International_Hotel_and_Tower_(Chicago)',\n",
        "  'Maya_stelae',\n",
        "  'Mary_Rose',\n",
        "  'Transandinomys_bolivaris',\n",
        "  'Plesiorycteropus',\n",
        "  'Ring-tailed_lemur',\n",
        "  'Javan_rhinoceros',\n",
        "  'Olympic_marmot',\n",
        "  'Taxonomy_of_lemurs',\n",
        "  'Northern_voalavo',\n",
        "  'Massospondylus',\n",
        "  'Heterodontosaurus',\n",
        "  'Argentinosaurus',\n",
        "  'Triceratops',\n",
        "  'Lock_Haven,_Pennsylvania',\n",
        "  'Anti-tobacco_movement_in_Nazi_Germany',\n",
        "  'Falkland_Islands',\n",
        "  'The_Catlins',\n",
        "  'Lesser_Antillean_macaw',\n",
        "  'Gigantorhynchus',\n",
        "  'Pacific_blue-eye',\n",
        "  'Cyclura_nubila',\n",
        "  'Margarita_with_a_Straw',\n",
        "  'Star_Trek_IV:_The_Voyage_Home',\n",
        "  'The_Simpsons_Movie',\n",
        "  'Star_Trek_Generations',\n",
        "  'Pah_Wongso_Pendekar_Boediman',\n",
        "  'Rings_of_Neptune',\n",
        "  'Hydrus',\n",
        "  'Corona_Borealis',\n",
        "  '1927_Chicago_mayoral_election',\n",
        "  'William_Henry_Harrison_1840_presidential_campaign',\n",
        "  'George_F._Kennan',\n",
        "  'William_Hayden_English',\n",
        "  'Moorgate_tube_crash',\n",
        "  'Interstate_355',\n",
        "  'SS_Washingtonian_(1913)',\n",
        "  'God_of_War:_Chains_of_Olympus',\n",
        "  'Lost_Luggage_(video_game)',\n",
        "  'Killer7',\n",
        "  'Final_Fantasy_VI',\n",
        "  'Final_Fantasy_Type-0',\n",
        "  'British_military_intervention_in_the_Sierra_Leone_Civil_War',\n",
        "  'Battle_of_Powick_Bridge',\n",
        "  'Battle_of_Pontvallain',\n",
        "  'Banksia_speciosa',\n",
        "  'Epacris_impressa',\n",
        "  'Miniopterus_griveaudi',\n",
        "  'Canada_lynx',\n",
        "  'Boulonnais_horse',\n",
        "  'American_Cream_Draft',\n",
        "  'Rufous-crowned_sparrow',\n",
        "  'Rock_parrot',\n",
        "  'Peregrine_falcon',\n",
        "  'Oceanic_whitetip_shark',\n",
        "  'Millipede',\n",
        "  'Australian_green_tree_frog',\n",
        "  'Cracker_Barrel',\n",
        "  'Harold_Innis',\n",
        "  'The_Million_Dollar_Homepage',\n",
        "  'PowerBook_100',\n",
        "  'The_Livestock_Conservancy',\n",
        "  'Gropecunt_Lane',\n",
        "  'Jack_the_Ripper:_The_Final_Solution',\n",
        "  'Apollo_9',\n",
        "  'Draining_and_development_of_the_Everglades',\n",
        "  'Mechanical_filter',\n",
        "  'Ernest_Emerson',\n",
        "  'Pisco_sour',\n",
        "  'Rogue_River_(Oregon)',\n",
        "  'Little_Butte_Creek',\n",
        "  'Doom_Bar',\n",
        "  'Columbia_River',\n",
        "  'North_Cascades_National_Park',\n",
        "  'Japan',\n",
        "  'Providence,_Rhode_Island',\n",
        "  'Mauna_Kea',\n",
        "  'Trafford_Park',\n",
        "  'Upper_and_Lower_Table_Rock',\n",
        "  'Fundamental_Rights,_Directive_Principles_and_Fundamental_Duties_of_India',\n",
        "  'Murder_of_Leigh_Leigh',\n",
        "  'Assassination_of_Robert_F._Kennedy',\n",
        "  '2013_Atlantic_hurricane_season',\n",
        "  '2002_Atlantic_hurricane_season',\n",
        "  '1944_Cuba–Florida_hurricane',\n",
        "  'Tropical_cyclone',\n",
        "  'Hurricane_Rick_(2009)',\n",
        "  'Lost_operas_by_Claudio_Monteverdi',\n",
        "  'Sentence_spacing',\n",
        "  'Eleanor_Rykener',\n",
        "  'Edward_Low',\n",
        "  'Geography_and_ecology_of_the_Everglades',\n",
        "  'Dorset',\n",
        "  'Gurian_Republic',\n",
        "  'Maple_syrup',\n",
        "  'Forest_Park_(Portland,_Oregon)',\n",
        "  'Ford_Island',\n",
        "  'Manchester',\n",
        "  'Knowle_West',\n",
        "  'Kigali',\n",
        "  'Sheerness',\n",
        "  'Roxy_Ann_Peak',\n",
        "  'Hudson_Valley_Rail_Trail',\n",
        "  'Buildings_and_architecture_of_Bristol',\n",
        "  'Benty_Grange_helmet',\n",
        "  'Holkham_Hall',\n",
        "  'Maiden_Castle,_Dorset',\n",
        "  'Saint_Fin_Barre%27s_Cathedral',\n",
        "  'Fauna_of_Puerto_Rico',\n",
        "  'Flora_of_Madagascar',\n",
        "  'The_Swimming_Hole',\n",
        "  'Rokeby_Venus',\n",
        "  'Streatham_portrait',\n",
        "  'The_Raft_of_the_Medusa',\n",
        "  'Portrait_of_a_Musician',\n",
        "  'The_Minute_Man',\n",
        "  'Pyramid_of_Unas',\n",
        "  'Shorwell_helmet',\n",
        "  'Pitfour_estate',\n",
        "  'Millennium_Park',\n",
        "  'House_with_Chimaeras',\n",
        "  'McCormick_Tribune_Plaza_%26_Ice_Rink',\n",
        "  'Geology_Hall,_New_Brunswick,_New_Jersey',\n",
        "  'Tower_of_London',\n",
        "  'Prince%27s_Palace_of_Monaco',\n",
        "  'Palace_of_Queluz',\n",
        "  'Nelson%27s_Pillar',\n",
        "  'Cloud_Gate',\n",
        "  'Denbies',\n",
        "  'Early_Netherlandish_painting',\n",
        "  'Las_Meninas',\n",
        "  'Portrait_Diptych_of_Dürer%27s_Parents',\n",
        "  'Portrait_of_a_Lady_(van_der_Weyden)',\n",
        "  'Portrait_of_Maria_Portinari',\n",
        "  'Portrait_of_Mariana_of_Austria',\n",
        "  'Migration_of_the_Serbs_(painting)',\n",
        "  'Armillaria_gallica',\n",
        "  'DNA_nanotechnology',\n",
        "  'Genetics',\n",
        "  'RNA_interference',\n",
        "  'Fungus',\n",
        "  'Morchella_rufobrunnea',\n",
        "  'Suillus_spraguei',\n",
        "  'Ramaria_botrytis',\n",
        "  'Suillus_luteus',\n",
        "  'Imperator_torosus',\n",
        "  'Paper_Mario:_The_Origami_King',\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lskYDGHbbqUE"
      },
      "source": [
        "def getPageData(page):\n",
        "  \"\"\"\n",
        "  Scrape a wikipedia page for all adjacent text/alternative text/image caption pairs\n",
        "  \"\"\"\n",
        "  soup = BeautifulSoup(page.html(), 'html.parser')\n",
        "  element = soup.div.div\n",
        "\n",
        "  Datum = namedtuple('Datum', 'adj_text, alt_text, img_caption, img_url')\n",
        "\n",
        "  page_data = []\n",
        "  while element and element.name not in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
        "    element = element.next_sibling\n",
        "\n",
        "  adj_text = ''\n",
        "  found_image = False\n",
        "\n",
        "  while element:\n",
        "    if element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
        "      if found_image:\n",
        "        page_data.append(Datum(adj_text, alt_text, img_caption, img_url))\n",
        "      found_image = False\n",
        "      adj_text = ''\n",
        "    if element.name == 'div' and not found_image:\n",
        "      img = element.find('img')\n",
        "      if img:\n",
        "        img_caption = element.text\n",
        "        alt_text = img.get('alt',None)\n",
        "        img_url = img.get('src', None)\n",
        "        if alt_text:\n",
        "          found_image = True\n",
        "    if element.name == 'p':\n",
        "      adj_text += element.text\n",
        "    element = element.next_sibling\n",
        "  return page_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD2x-9-Yd0m6"
      },
      "source": [
        "\"\"\"\n",
        "Strategies\n",
        "\"\"\"\n",
        "\n",
        "class Strategy:\n",
        "  \"\"\"\n",
        "  Class to call the methods as strings\n",
        "  \"\"\"\n",
        "\n",
        "  def adj(self, page, datum):\n",
        "    \"\"\"\n",
        "    Get Adjacent text of image\n",
        "    \"\"\"\n",
        "    return datum.adj_text\n",
        "\n",
        "  def tit(self, page, datum):\n",
        "    \"\"\"\n",
        "    Get page title\n",
        "    \"\"\"\n",
        "    return page.title\n",
        "\n",
        "  def sum(self, page, datum):\n",
        "    \"\"\"\n",
        "    Get page summary\n",
        "    \"\"\"\n",
        "    return page.summary\n",
        "\n",
        "  def all(self, page, datum):\n",
        "    \"\"\"\n",
        "    Get all text on a page\n",
        "    \"\"\"\n",
        "    return page.content\n",
        "\n",
        "  def alt(self, page, datum):\n",
        "    \"\"\"\n",
        "    Get image alternative text\n",
        "    \"\"\"\n",
        "    return datum.alt_text\n",
        "\n",
        "  def cmp_vsn(self, page, datum):\n",
        "    description_results = computervision_client.describe_image('http:'+datum.img_url)\n",
        "    if len(description_results.captions) == 0:\n",
        "      return \"No description detected.\"\n",
        "    return description_results.captions[0].text\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUhrvzlFiO5w"
      },
      "source": [
        "def get_content(page, datum, input_strategies, output_strategies):\n",
        "  \"\"\"\n",
        "  Get input and out using various strategies\n",
        "  \"\"\"\n",
        "  input = ''\n",
        "  output = ''\n",
        "\n",
        "  m = globals()['Strategy']()\n",
        "\n",
        "  for strategy in input_strategies:\n",
        "    input += getattr(m, strategy)(page, datum)\n",
        "  for strategy in output_strategies:\n",
        "    output += getattr(m, strategy)(page, datum)\n",
        "  return input, output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nitBQUqN0zQG"
      },
      "source": [
        "\"\"\"\n",
        "Various Helper methods for our pipelines\n",
        "\"\"\"\n",
        "\n",
        "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n",
        "\n",
        "def flip_entities(entities_dict):\n",
        "  \"\"\"\n",
        "  Flip a dictionary to include value:key pairs from\n",
        "  key:value pairs\n",
        "  \"\"\"\n",
        "  result = {}\n",
        "  for key, val in entities_dict.items():\n",
        "    if len(val) > 0:\n",
        "      result[key] = val\n",
        "      for v in val:\n",
        "        x = [*val, key]\n",
        "        x.remove(v)\n",
        "        result[v] = x\n",
        "  return result\n",
        "\n",
        "def getRelatedWords(adjacent_text):\n",
        "  \"\"\"\n",
        "  Get words which are mentioned together from the\n",
        "  Google entity API\n",
        "  \"\"\"\n",
        "  url = \"https://language.googleapis.com/v1/documents:analyzeEntities?key=AIzaSyDFgIPD1fYHXk3FFcvN8TMw6SrSfaM7wbY\"\n",
        "  json = {\n",
        "    \"document\":{\n",
        "      \"type\":\"PLAIN_TEXT\",\n",
        "      \"content\": adjacent_text\n",
        "    },\n",
        "    \"encodingType\":\"UTF8\"\n",
        "  }\n",
        "  response = requests.post(url, json=json)\n",
        "  entities = response.json()['entities']\n",
        "  entities_dict = {entity[\"name\"].lower():[x['text']['content'] for x in entity['mentions'] if x['text']['content'] != entity[\"name\"].lower()] for entity in entities if len(entity['mentions']) > 1}\n",
        "  return flip_entities(entities_dict)\n",
        "\n",
        "def preprocess(sent):\n",
        "    \"\"\"\n",
        "    Preprocess a given text to extract out the Nouns\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english')) \n",
        "    word_tokens = word_tokenize(sent.lower()) \n",
        "    sent = [w for w in word_tokens if not w in stop_words]\n",
        "    sent = nltk.pos_tag(sent)\n",
        "    return [x[0] for x in sent if 'NN' in x[1]]\n",
        "\n",
        "def vec(s):\n",
        "    \"\"\"\n",
        "    Convert a word to a given vector\n",
        "    \"\"\"\n",
        "    return nlp.vocab[s].vector\n",
        "\n",
        "def embed_sentence(s):\n",
        "    \"\"\"\n",
        "    Convert a word to a given vector\n",
        "    \"\"\"\n",
        "    return sbert_model.encode([s])[0]\n",
        "\n",
        "def cosine(v1, v2):\n",
        "    \"\"\"\n",
        "    Find closest vectors using the cosine rule\n",
        "    \"\"\"\n",
        "    if norm(v1) > 0 and norm(v2) > 0:\n",
        "        return dot(v1, v2) / (norm(v1) * norm(v2))\n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "def spacy_closest(token_list, vec_to_check, n=10):\n",
        "    \"\"\"\n",
        "    Returns the n closest vectors to a given vector\n",
        "    \"\"\"\n",
        "    return sorted(token_list,\n",
        "                  key=lambda x: cosine(vec_to_check, vec(x)),\n",
        "                  reverse=True)[:n]\n",
        "\n",
        "def bert_closest(token_list, vec_to_check):\n",
        "    \"\"\"\n",
        "    Returns the n closest vectors to a given vector\n",
        "    \"\"\"\n",
        "    return sorted(token_list,\n",
        "                  key=lambda x: cosine(vec_to_check, embed_sentence(x)),\n",
        "                  reverse=True)[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLAVYTiN0S59"
      },
      "source": [
        "def nounPipeline(input_content, output_content):\n",
        "  \"\"\"\n",
        "  Pipeline for swapping nouns using the noun swapping approach\n",
        "  \"\"\"\n",
        "  image_description_tags = preprocess(output_content)\n",
        "  adjacent_text_tags = preprocess(input_content)\n",
        "\n",
        "  entities_dict = getRelatedWords(input_content)\n",
        "  result_text = output_content\n",
        "  for img_tag in image_description_tags:\n",
        "    if img_tag in entities_dict:\n",
        "      result_text = result_text.replace(img_tag, random.choice(entities_dict[img_tag]))\n",
        "  return result_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQqQ25sU5jei"
      },
      "source": [
        "def phrasePipeline(input_content, output_content):\n",
        "  \"\"\"\n",
        "  Pipeline for swapping key phrases using the key phrases swapping approach\n",
        "  \"\"\"\n",
        "  image_description_tags = preprocess(output_content)\n",
        "  entities_dict = getRelatedWords(input_content)\n",
        "\n",
        "  r = Rake() \n",
        "  r.extract_keywords_from_text(input_content)\n",
        "  scored = r.get_ranked_phrases_with_scores() \n",
        "\n",
        "  result_text = output_content\n",
        "  for img_tag in image_description_tags:\n",
        "    max_for_tag = -1\n",
        "    best_phrase = None\n",
        "    if img_tag in entities_dict:\n",
        "      for val in entities_dict[img_tag]:\n",
        "        for rank,phrase in scored:\n",
        "          if val in phrase and rank > max_for_tag:\n",
        "            max_for_tag = rank\n",
        "            best_phrase = phrase\n",
        "    if best_phrase:\n",
        "      result_text = result_text.replace(img_tag,best_phrase)\n",
        "  return result_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyjN9VE04nj1"
      },
      "source": [
        "def embeddingPipeline(input_content, output_content):\n",
        "  \"\"\"\n",
        "  Pipeline for swapping words based on their word embedding proximity\n",
        "  \"\"\"\n",
        "  doc_adj = nlp(input_content)\n",
        "  tokens = list([w.text for w in doc_adj if w.is_alpha])\n",
        "\n",
        "  r = Rake() \n",
        "  r.extract_keywords_from_text(output_content)\n",
        "  scored_img = r.get_ranked_phrases_with_scores() \n",
        "\n",
        "  r.extract_keywords_from_text(input_content)\n",
        "  scored_adj = r.get_ranked_phrases_with_scores()\n",
        "\n",
        "  result_text = output_content.lower()\n",
        "\n",
        "  for phrase in scored_img:\n",
        "    avg_vector = np.mean([vec(w) for w in phrase[1].split(' ')], axis=0)\n",
        "    closest = spacy_closest(tokens, avg_vector, 25)\n",
        "    for score in scored_adj:\n",
        "      for close in closest:\n",
        "        found = False\n",
        "        if close in score[1]:\n",
        "          result_text = result_text.replace(phrase[1], score[1])\n",
        "          found = True\n",
        "          break\n",
        "      if found:\n",
        "        break\n",
        "\n",
        "  tool.check(result_text)\n",
        "  result_text = tool.correct(result_text)\n",
        "  return result_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uintjn8CAHx-"
      },
      "source": [
        "def sentenceEmbeddingPipeline(input_content, output_content):\n",
        "  \"\"\"\n",
        "  Pipeline for finding closest sentence int the input content\n",
        "  to the output content\n",
        "  \"\"\"\n",
        "  sentences = tokenize.sent_tokenize(input_content)\n",
        "  query_embedding = sbert_model.encode([output_content])[0]\n",
        "  return bert_closest(sentences, query_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC_aqOXS1Wqn"
      },
      "source": [
        "def openAIPipeline(input_content, image_url):\n",
        "  image = openai_preprocess(Image.open(requests.get(image_url, stream=True).raw)).unsqueeze(0).to(device)\n",
        "  sentences = re.split('[?.,]',input_content)\n",
        "  text = clip.tokenize(sentences).to(device)\n",
        "  with torch.no_grad():\n",
        "    image_features = openai_model.encode_image(image)\n",
        "    text_features = openai_model.encode_text(text)\n",
        "    \n",
        "    logits_per_image, logits_per_text = openai_model(image, text)\n",
        "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "  return sentences[np.argmax(probs)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIrL9Fle6nuG"
      },
      "source": [
        "def getImageCategories(image_url):\n",
        "    remote_image_features = [\"categories\"]\n",
        "    category_results = computervision_client.analyze_image(image_url , remote_image_features)\n",
        "    categories = {\n",
        "        'animal': 0,\n",
        "        'building': 0,\n",
        "        'indoor': 0,\n",
        "        'outdoor': 0,\n",
        "        'people': 0,\n",
        "    }\n",
        "    for category in category_results.categories:\n",
        "      base_category = category.name.split('_')[0]\n",
        "      if base_category in categories:\n",
        "        categories[base_category] = 1\n",
        "    return categories\n",
        "\n",
        "def getImageDescription(image_url):\n",
        "    description_results = computervision_client.describe_image(image_url)\n",
        "    if len(description_results.captions) == 0:\n",
        "      return \"No description detected.\", 0\n",
        "    return description_results.captions[0].confidence, description_results.captions[0].text\n",
        "\n",
        "def AggregatePipeline(input_content, output_content, image_url):\n",
        "  strategy = Strategy()\n",
        "  image = openai_preprocess(Image.open(requests.get(image_url, stream=True).raw)).unsqueeze(0).to(device)\n",
        "  input_content += nounPipeline(input_content, output_content) + \". \"\n",
        "  input_content += phrasePipeline(input_content, output_content) + \". \"\n",
        "  input_content += embeddingPipeline(input_content, output_content) + \". \"\n",
        "  input_content += sentenceEmbeddingPipeline(input_content, output_content) + \". \"\n",
        "  sentences = re.split('[?.,]',input_content)\n",
        "  sentences.append(output_content)\n",
        "  text = clip.tokenize(sentences).to(device)\n",
        "  with torch.no_grad():\n",
        "    image_features = openai_model.encode_image(image)\n",
        "    text_features = openai_model.encode_text(text)\n",
        "    logits_per_image, logits_per_text = openai_model(image, text)\n",
        "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "    prob, sentence = sorted(zip(probs[0], sentences))[::-1][0]\n",
        "    index = sentences.index(output_content)\n",
        "    output_prob = probs[0][index]\n",
        "  return prob, sentence, output_prob, output_content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MuyAvRLH7Zc"
      },
      "source": [
        "def scoreOutput(result, caption):\n",
        "  result_embedding = sbert_model.encode([result])[0]\n",
        "  caption_embedding = sbert_model.encode([caption])[0]\n",
        "  return cosine(result_embedding, caption_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqwuc4Hbhkd2",
        "outputId": "2eebf242-507e-4308-be59-5268b6e73e47"
      },
      "source": [
        "\"\"\"\n",
        "Main script to call our pipelines on the different datapoints\n",
        "Input Content Strategies: \n",
        "  -adj (adjacent text)\n",
        "  -tit (page title)\n",
        "  -sum (page summary)\n",
        "  -all (all page content)\n",
        "\n",
        "Output Content Strategies: \n",
        "  -alt (image alternative text)\n",
        "\"\"\"\n",
        "\n",
        "class style:\n",
        "   BOLD = '\\033[1m'\n",
        "   END = '\\033[0m'\n",
        "\n",
        "with open('AggregateData.csv', mode='w') as csv_file:\n",
        "  fieldnames = ['OpenAI', 'Azure', 'Caption', \"Label\",'OpenAI_Conf', 'OpenAI_Azure_Conf', 'Azure_Conf', 'Animal', 'Building', 'Indoor', 'Outdoor', 'People']\n",
        "  writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "  writer.writeheader()\n",
        "  for page_name in page_names:\n",
        "    try:\n",
        "      page = p = wikipedia.page(page_name)\n",
        "      data = getPageData(page)\n",
        "      print(page_name)\n",
        "      for datum in data:\n",
        "        try:        \n",
        "          input_content, output_content = get_content(page, datum, ['adj'], ['cmp_vsn'])\n",
        "      \n",
        "          aggregate_prob, aggregate_description, aggregate_azure_prob, _ = AggregatePipeline(input_content, output_content, 'https:' + datum.img_url)\n",
        "          azure_prob, azure_description = getImageDescription('https:' + datum.img_url)\n",
        "          categories = getImageCategories('https:' + datum.img_url)\n",
        "          \n",
        "          writer.writerow({\n",
        "              'OpenAI': aggregate_description, \n",
        "              'Azure': azure_description,\n",
        "              'Caption': datum.img_caption,\n",
        "              'Label': '',\n",
        "              'OpenAI_Conf': aggregate_prob, \n",
        "              'OpenAI_Azure_Conf': aggregate_azure_prob, \n",
        "              'Azure_Conf': azure_prob, \n",
        "              'Animal': categories['animal'], \n",
        "              'Building': categories['building'], \n",
        "              'Indoor': categories['indoor'], \n",
        "              'Outdoor': categories['outdoor'], \n",
        "              'People': categories['people'], \n",
        "          })\n",
        "        except:\n",
        "          pass\n",
        "    except:\n",
        "      pass    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Egyptian_temple\n",
            "Connecticut_Tercentenary_half_dollar\n",
            "Eastern_green_mamba\n",
            "Barack\n",
            "Mount_Tambora\n",
            "Hans_Eberle\n",
            "Fennel\n",
            "3rd_Light_Horse_Brigade\n",
            "Liberal_feminism\n",
            "Presidency_of_Joe_Biden\n",
            "Blacksmith\n",
            "7_World_Trade_Center\n",
            "Michigan_State_Capitol\n",
            "Santa_María_de_Óvila\n",
            "Felice_Beato\n",
            "Clathrus_ruber\n",
            "Green_rosella\n",
            "Psittacosaurus\n",
            "Lafayette_dollar\n",
            "Ficus_obliqua\n",
            "Hu_Zhengyan\n",
            "Olmec_colossal_heads\n",
            "Warkworth_Castle\n",
            "Carnivàle\n",
            "H.M.S._Pinafore\n",
            "Magnetosphere_of_Jupiter\n",
            "2015_Formula_One_World_Championship\n",
            "Ruma_Maida\n",
            "Wells_Cathedral\n",
            "Introduction_to_viruses\n",
            "Salvia_yangii\n",
            "Psilocybe_aztecorum\n",
            "Halkett_boat\n",
            "Rolls-Royce_R\n",
            "Titchwell_Marsh\n",
            "Liber_Eliensis\n",
            "Imaginative_Tales\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "March_2021_Australian_floods\n",
            "Justice_League_(film)\n",
            "The_Falcon_and_the_Winter_Soldier\n",
            "Billie_Eilish\n",
            "Nine_Stones,_Winterbourne_Abbas\n",
            "Palazzo_Pitti\n",
            "Sloan–Parker_House\n",
            "Mycena_haematopus\n",
            "Lycoperdon_echinatum\n",
            "Entoloma_sinuatum\n",
            "Gyromitra_esculenta\n",
            "Banksia_verticillata\n",
            "Banksia_brownii\n",
            "Ficus_macrophylla\n",
            "Banksia_aculeata\n",
            "Columbian_mammoth\n",
            "Evolution_of_lemurs\n",
            "Ferugliotherium\n",
            "Orangutan\n",
            "Thomasomys_ucucha\n",
            "Garden_warbler\n",
            "Puerto_Rican_amazon\n",
            "Inaccessible_Island_rail\n",
            "Echo_parakeet\n",
            "Russet_sparrow\n",
            "Pied_butcherbird\n",
            "King_Island_emu\n",
            "California_condor\n",
            "Green_children_of_Woolpit\n",
            "Polish_culture_during_World_War_II\n",
            "Kylfings\n",
            "British_National_(Overseas)\n",
            "Baden-Powell_House\n",
            "Anna_Anderson\n",
            "Royal_National_College_for_the_Blind\n",
            "Construction_of_the_World_Trade_Center\n",
            "Shale_oil_extraction\n",
            "Shuttle–Mir_program\n",
            "Nico_Ditch\n",
            "Payún_Matrú\n",
            "Larrys_Creek\n",
            "1966_New_York_City_smog\n",
            "Brown_Dog_affair\n",
            "École_Polytechnique_massacre\n",
            "The_Magazine_of_Fantasy_%26_Science_Fiction\n",
            "Future_Science_Fiction_and_Science_Fiction_Stories\n",
            "Famous_Fantastic_Mysteries\n",
            "Super_Smash_Bros._Brawl\n",
            "God_of_War:_Betrayal\n",
            "The_Simpsons_Game\n",
            "Defense_of_the_Ancients\n",
            "Honan_Chapel\n",
            "Gevninge_helmet_fragment\n",
            "Clemuel_Ricketts_Mansion\n",
            "Trump_International_Hotel_and_Tower_(Chicago)\n",
            "Maya_stelae\n",
            "Mary_Rose\n",
            "Transandinomys_bolivaris\n",
            "Plesiorycteropus\n",
            "Ring-tailed_lemur\n",
            "Javan_rhinoceros\n",
            "Olympic_marmot\n",
            "Taxonomy_of_lemurs\n",
            "Northern_voalavo\n",
            "Massospondylus\n",
            "Heterodontosaurus\n",
            "Argentinosaurus\n",
            "Triceratops\n",
            "Anti-tobacco_movement_in_Nazi_Germany\n",
            "Falkland_Islands\n",
            "The_Catlins\n",
            "Lesser_Antillean_macaw\n",
            "Gigantorhynchus\n",
            "Cyclura_nubila\n",
            "Margarita_with_a_Straw\n",
            "The_Simpsons_Movie\n",
            "Star_Trek_Generations\n",
            "Pah_Wongso_Pendekar_Boediman\n",
            "Rings_of_Neptune\n",
            "Hydrus\n",
            "Corona_Borealis\n",
            "1927_Chicago_mayoral_election\n",
            "William_Henry_Harrison_1840_presidential_campaign\n",
            "William_Hayden_English\n",
            "Moorgate_tube_crash\n",
            "Interstate_355\n",
            "SS_Washingtonian_(1913)\n",
            "God_of_War:_Chains_of_Olympus\n",
            "Killer7\n",
            "Final_Fantasy_Type-0\n",
            "British_military_intervention_in_the_Sierra_Leone_Civil_War\n",
            "Battle_of_Powick_Bridge\n",
            "Battle_of_Pontvallain\n",
            "Banksia_speciosa\n",
            "Epacris_impressa\n",
            "Miniopterus_griveaudi\n",
            "Canada_lynx\n",
            "Boulonnais_horse\n",
            "American_Cream_Draft\n",
            "Rufous-crowned_sparrow\n",
            "Rock_parrot\n",
            "Peregrine_falcon\n",
            "Oceanic_whitetip_shark\n",
            "Millipede\n",
            "Australian_green_tree_frog\n",
            "Cracker_Barrel\n",
            "Harold_Innis\n",
            "The_Million_Dollar_Homepage\n",
            "PowerBook_100\n",
            "The_Livestock_Conservancy\n",
            "Gropecunt_Lane\n",
            "Apollo_9\n",
            "Draining_and_development_of_the_Everglades\n",
            "Mechanical_filter\n",
            "Ernest_Emerson\n",
            "Pisco_sour\n",
            "Little_Butte_Creek\n",
            "Columbia_River\n",
            "North_Cascades_National_Park\n",
            "Japan\n",
            "Providence,_Rhode_Island\n",
            "Mauna_Kea\n",
            "Trafford_Park\n",
            "Upper_and_Lower_Table_Rock\n",
            "Fundamental_Rights,_Directive_Principles_and_Fundamental_Duties_of_India\n",
            "Murder_of_Leigh_Leigh\n",
            "Assassination_of_Robert_F._Kennedy\n",
            "2013_Atlantic_hurricane_season\n",
            "2002_Atlantic_hurricane_season\n",
            "Tropical_cyclone\n",
            "Lost_operas_by_Claudio_Monteverdi\n",
            "Sentence_spacing\n",
            "Eleanor_Rykener\n",
            "Edward_Low\n",
            "Geography_and_ecology_of_the_Everglades\n",
            "Dorset\n",
            "Gurian_Republic\n",
            "Maple_syrup\n",
            "Ford_Island\n",
            "Manchester\n",
            "Knowle_West\n",
            "Kigali\n",
            "Roxy_Ann_Peak\n",
            "Hudson_Valley_Rail_Trail\n",
            "Buildings_and_architecture_of_Bristol\n",
            "Benty_Grange_helmet\n",
            "Holkham_Hall\n",
            "Maiden_Castle,_Dorset\n",
            "Saint_Fin_Barre%27s_Cathedral\n",
            "Fauna_of_Puerto_Rico\n",
            "Flora_of_Madagascar\n",
            "The_Swimming_Hole\n",
            "Rokeby_Venus\n",
            "Streatham_portrait\n",
            "The_Raft_of_the_Medusa\n",
            "Portrait_of_a_Musician\n",
            "The_Minute_Man\n",
            "Pyramid_of_Unas\n",
            "Shorwell_helmet\n",
            "Pitfour_estate\n",
            "Millennium_Park\n",
            "House_with_Chimaeras\n",
            "McCormick_Tribune_Plaza_%26_Ice_Rink\n",
            "Tower_of_London\n",
            "Prince%27s_Palace_of_Monaco\n",
            "Palace_of_Queluz\n",
            "Nelson%27s_Pillar\n",
            "Denbies\n",
            "Early_Netherlandish_painting\n",
            "Portrait_Diptych_of_Dürer%27s_Parents\n",
            "Portrait_of_Maria_Portinari\n",
            "Portrait_of_Mariana_of_Austria\n",
            "Migration_of_the_Serbs_(painting)\n",
            "Armillaria_gallica\n",
            "DNA_nanotechnology\n",
            "RNA_interference\n",
            "Fungus\n",
            "Morchella_rufobrunnea\n",
            "Suillus_spraguei\n",
            "Ramaria_botrytis\n",
            "Suillus_luteus\n",
            "Imperator_torosus\n",
            "Paper_Mario:_The_Origami_King\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeKpCw-tMA7Z",
        "outputId": "53b5569e-6544-4a3e-b8a5-b24f7179ef60"
      },
      "source": [
        "\"\"\"\n",
        "Final Model\n",
        "\"\"\"\n",
        "result = {}\n",
        "loaded_model = pickle.load(open('finalized_model.sav', 'rb'))\n",
        "page_name = \"Paper_Mario:_The_Origami_King\"\n",
        "page = wikipedia.page(page_name)\n",
        "data = getPageData(page)\n",
        "for datum in data:\n",
        "  try:      \n",
        "    input_content, output_content = get_content(page, datum, ['adj'], ['cmp_vsn'])\n",
        "\n",
        "    aggregate_prob, aggregate_description, aggregate_azure_prob, _ = AggregatePipeline(input_content, output_content, 'https:' + datum.img_url)\n",
        "    azure_prob, azure_description = getImageDescription('https:' + datum.img_url)\n",
        "    categories = getImageCategories('https:' + datum.img_url)\n",
        "    prediction = loaded_model.predict(\n",
        "        [[\n",
        "          aggregate_prob,\n",
        "          aggregate_azure_prob,\n",
        "          azure_prob,\n",
        "          categories['animal'],\n",
        "          categories['building'],\n",
        "          categories['indoor'],\n",
        "          categories['outdoor'],\n",
        "          categories['people'],\n",
        "        ]]\n",
        "    )\n",
        "    result['https:' + datum.img_url] = aggregate_description if prediction == 0 else azure_description\n",
        "  except:\n",
        "    pass\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'https://upload.wikimedia.org/wikipedia/en/thumb/2/2f/Ring_Style_Combat.jpg/220px-Ring_Style_Combat.jpg': ' Mario stands at the center of a dartboard-like arena divided into four ring-shaped sections with 12 radial slots; each enemy occupies a different slot', 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/07/Kensuke_Tanabe_at_E3.png/180px-Kensuke_Tanabe_at_E3.png': 'Kensuke Tanabe wearing glasses and a red tie'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQjlSf0cOjuE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}